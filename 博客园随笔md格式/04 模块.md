##### 1.列举常用的模块。
```python
基础：
os,sys,time,datetime,json,pickle,randon,hashlib,re,math,logging
爬虫：
requests,BeautifulSoup,xpath,gevent,asyncio,twisted
数据分析：
pandas,numpy,scipy,matplotlib,seaborn

等。。。
```
##### 2.如何安装第三方模块?
```python
pip3 install 模块名称
```
##### 3.re 的 match 和 search 区别?
match()函数只检测字符串开头位置是否匹配，匹配成功才会返回结果，否则返回None
```python
import re

s1 = 'abcabcabc'

print(re.match('abc', s1))
print(re.match('abc', s1).group())
print(re.match('abc', s1).span())
```
search()函数会在整个字符串内查找模式匹配,只到找到第一个匹配然后返回一个包含匹配信息的对象,该对象可以通过调用group()方法得到匹配的字符串,如果字符串没有匹配，则返回None。
```python
import re

s1 = 'abcabcabc'

print(re.search('bca', s1))
print(re.search('bca', s1).group())
print(re.search('bca', s1).span())
```
##### 4.什么是正则的贪婪匹配?或 正则匹配中的贪婪模式与非贪婪模式的区别?
贪婪模式：能匹配的最大部分
“.+”中的+会一直查找到最后一个符合条件的字符
非贪婪模式：匹配越少越好
“.+?"会匹配到第一个符合条件的字符就停止
##### 5.如何生成一个随机数?
```python
import random
random.randint(1,9)
```
##### 6.如何使用 python 删除一个文件?
```python
import os
os.remove('文件名')
```
##### 7.logging 模块的作用?以及应用场景?
```python
在我们平时编写的程序，基本上都会有记录日志的需求，并且日志当中包含的内容既有正常的程序访问，又有错误，警告等信息输出，在python的logging模块当中就提供了标准的日志接口，可以通过它来存储各种格式的日志.
logging模块也分为五个等级：
debug(),
info(),
warning(),
error(),
critical()
其中默认级别为warning，默认打印到终端

"""
开发应用程序或部署开发环境时，可以使用DEBUG或INFO级别的日志获取尽可能详细的日志信息来进行开发或部署调试；应用上线或部署生产环境时，应该使用WARNING或ERROR或CRITICAL级别的日志来降低机器的I/O压力和提高获取错误日志信息的效率。
"""
```
##### 8.json 序列化时，可以处理的数据类型有哪些?如何定制支持 datetime 类 型?
```python
可以处理的数据类型是 string、int、list、tuple、dict、bool、null

# 自定义时间序列化转换器
import json
from json import JSONEncoder
from datetime import datetime
class ComplexEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.strftime('%Y-%m-%d %H:%M:%S')
        else:
            return super(ComplexEncoder,self).default(obj)
d = { 'name':'alex','data':datetime.now()}
print(json.dumps(d,cls=ComplexEncoder))
```
##### 9.json 序列化时，默认遇到中文会转换成 unicode，如果想要保留中文怎么办?
```python
import json
a=json.dumps({"ddf":"你好"},ensure_ascii=False)
print(a) #{"ddf": "你好"}
```
##### 10.写代码实现查看一个目录下的所有文件。
```python
import os

print(os.listdir(path))
```
##### 11.用Python匹配HTMLtag的时候,`<.*>` 和 `<.*?>`有什么区别?
```python
<.*>:贪婪匹配，会匹配到符合条件的所有标签
<.*?>:非贪婪匹配，匹配到符合条件的第一个标签
```
##### 12.如何判断一个邮箱合法
```python
import re

if re.match(r'^[a-zA-Z0-9_-]+@[a-zA-Z0-9_-]+(\.[a-zA-Z0-9_-]+)+$', email):
    print('right')
else:
    print('wrong')
```
##### 13.请写出以字母或下划线开始,以数字结束的正则表达式
##### 14.下面那些是Python开发网络应用的框架
```
1.  Django
2.  Yii
3.  Struts
4.  Rails
5.  Uliweb
6.  CodeIgniter
7.  gevent
8.  Flask
9.  web2py
10.  Pylons
11.  Tornado
12.  Twisted
13.  TurboGears

1,8,9,11,12,13
```
##### 15.写Python爬虫都用到了那些模块,分别是做什么用的?
```python
requests：请求库
BeautifulSoup：解析库
selenium：自动化测试工具
pyquery：网页解析库
pymysql：存储库。操作mysql数据库的
pymongo：操作mongodb数据库的
urllib.request 请求模块  　　模拟浏览器
urllib.error 异常处理模块
urllib.parse url解析模块  　　工具模块，如：拆分、合并
urllib.robotparser robots.txt    解析模块　　
```
##### 16.sys.path.append("/root/mods")的作用?
将/root/mods文件添加到环境变量
##### 17.列举出Python中比较熟知的爬虫框架
```python
1.Scrapy

Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。。用这个框架可以轻松爬下来如亚马逊商品信息之类的数据。

2.PySpider

pyspider 是一个用python实现的功能强大的网络爬虫系统，能在浏览器界面上进行脚本的编写，功能的调度和爬取结果的实时查看，后端使用常用的数据库进行爬取结果的存储，还能定时设置任务与任务优先级等。

3.Crawley

Crawley可以高速爬取对应网站的内容，支持关系和非关系数据库，数据可以导出为JSON、XML等。

4.Portia

Portia是一个开源可视化爬虫工具，可让您在不需要任何编程知识的情况下爬取网站！简单地注释您感兴趣的页面，Portia将创建一个蜘蛛来从类似的页面提取数据。

5.Newspaper

Newspaper可以用来提取新闻、文章和内容分析。使用多线程，支持10多种语言等。

6.Cola

Cola是一个分布式的爬虫框架，对于用户来说，只需编写几个特定的函数，而无需关注分布式运行的细节。任务会自动分配到多台机器上，整个过程对用户是透明的。

7.Grab

Grab是一个用于构建Web刮板的Python框架。借助Grab，您可以构建各种复杂的网页抓取工具，从简单的5行脚本到处理数百万个网页的复杂异步网站抓取工具。Grab提供一个API用于执行网络请求和处理接收到的内容，例如与HTML文档的DOM树进行交互。
```
##### 18.输入某年某日,判断这是这一年的第几天?(可以用Python的内置模块)
```python
import datetime

y = int(input('请输入4位数字的年份：'))  # 获取年份
m = int(input('请输入月份：'))  # 获取月份
d = int(input('请输入是哪一天：'))  # 获取“日”

targetDay = datetime.date(y, m, d)  # 将输入的日期格式化成标准的日期
dayCount = targetDay - datetime.date(targetDay.year - 1, 12, 31)  # 减去上一年最后一天
print('%s是%s年的第%s天。' % (targetDay, y, dayCount.days))
```
##### 19.使用过Python那些第三方组件?